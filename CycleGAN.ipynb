{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CycleGAN_tutorial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arokem/IntroDL/blob/master/CycleGAN_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irY2vJRElXsX",
        "colab_type": "text"
      },
      "source": [
        "# Code example: CycleGAN \n",
        "\n",
        "This code example provides a full implementation of CycleGAN in Keras. It is is based on [this implementation](https://github.com/simontomaskarlsson/CycleGAN-Keras) by Simon Karlsson. Although it is possible to run this code on a CPU, a computer with a **GPU is strongly recommended**. The code is provided ready to run, but also includes multiple adjustable settings.\n",
        "\n",
        "#### What is CycleGAN?\n",
        "CycleGAN is an unsupervised image-to-image translation architecture proposed in 2017 by [Zhu et al.](https://arxiv.org/abs/1703.10593) Its most remarkable feature is its capacity for learning mappings between classes of images without requiring paired data, making it something of a \"universal image translator\". This is achieved by complementing the regular adversarial losses seen in GANs with \"cycle consistency\" losses, which enforce $$A \\stackrel{G_{AB}}{\\rightarrow} B' \\stackrel{G_{BA}}{\\rightarrow} A'' \\approx A $$ and $$B \\stackrel{G_{BA}}{\\rightarrow} A' \\stackrel{G_{AB}}{\\rightarrow} B'' \\approx B . $$\n",
        "\n",
        "#### Directory structure:\n",
        "- **data/** contains the datasets.\n",
        "    - **data/&lt;dataset&gt;/{train_A, train_B}/** contains training images for classes A and B.\n",
        "    - **data/&lt;dataset&gt;/{test_A, test_B}/** contains testing images that are not used during training. These are useful to evaluate the generalization of the model to new data.\n",
        "- **images/** stores metadata and loss information of each CycleGAN run, as well as evaluation images.\n",
        "    - **images/meta_data.json** contains the settings of the run.\n",
        "    - **images/loss_output.csv** contains the various losses of the model, stored after every batch.\n",
        "    - **images/{train_A, train_B, test_A, test_B}** contains intermediate evaluation images for each epoch, illustrating generator performance.\n",
        "    - **images/tmp.png** shows example image translations from the current moment in training. This image updates in real time and can be used to see how the training converges.\n",
        "- **saved_models** stores the generator and discriminator models resulting from each run, which are saved every 20 epochs.\n",
        "\n",
        "#### Example data\n",
        "We provide a small example dataset with images of male and female faces. During training the CycleGAN learns to switch the genre of the faces. This dataset is small enough that the training can be run in under 15 min with a standard GPU. This allows visualization of the training progess in real time by montitoring the **images/tmp.png** file.  New datasets can be added by placing them in the **data/** folder, and can be selected by setting the `image_folder` variable below.\n",
        "\n",
        "#### Interpretation of output images\n",
        "- **images/tmp.png** has two rows. The top row shows, from left to right, the original image $A$, the translated image $B'=G_{AB}(A)$ and the recovered image $A'' = G_{BA}(B') = G_{BA}(G_{AB}(A))$. The bottom row shows similar images for the other domain: $B$, $A'$, $B''$. Here is an exmaple form the middle of training:\n",
        "\n",
        "    <img src=\"https://github.com/brainhack101/IntroDL/blob/master/notebooks/2019/Eklund/notebook_images/tmp.png?raw=1\">\n",
        "\n",
        "    The adversarial losses push the middle image in both rows to look realistic. On the other hand, the cycle consistenxy losses force the left (original) and right (reconstructed) images to be similar.\n",
        "\n",
        "- **images/{train_A, ..., test_B}** contains example results for each training epoch. If the dataset is _unpaired_ it is essentially the same as **tmp.png**:\n",
        "\n",
        "    <img src=\"https://github.com/brainhack101/IntroDL/blob/master/notebooks/2019/Eklund/notebook_images/MFepoch200.png?raw=1\">\n",
        "    \n",
        "    If the data is _paired_ a new image is added in the first position representing the ground truth for the conversion. Here is an example conversion between T1w and T2w MRI images (these images are paired because the T1w and T2w are of the same subject):\n",
        "    \n",
        "    <img src=\"https://github.com/brainhack101/IntroDL/blob/master/notebooks/2019/Eklund/notebook_images/T2T1epoch10.png?raw=1\">\n",
        "\n",
        "    In this scenario the first image should match the third and the second should match the fourth. From left to right the imageas are $B_{GT}$, $A$, $B'$, $A''$. Note that the conversion from $A$ to $B$ matches well with the ground truth despite the fact that CycleGAN is unaware that the data is paired."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEyU05yplivE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -nc https://raw.githubusercontent.com/brainhack101/IntroDL/master/notebooks/2019/Eklund/requirements-gpu.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF-Dt-UMl8aV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -r requirements-gpu.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWUFie0RlXsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Layer, Input, Dropout, Conv2D, Activation, add, UpSampling2D, \\\n",
        "    Conv2DTranspose, Flatten\n",
        "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization, InputSpec\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.core import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.engine.topology import Network\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from progress.bar import Bar\n",
        "import datetime\n",
        "import time\n",
        "import json\n",
        "import csv\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import keras.backend as K\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l6S9SEKlXsi",
        "colab_type": "text"
      },
      "source": [
        "Additional functions are contained in the `helper_functions.py` file. These mostly include code for loading the data and saving the resutls."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whlevvwXnWrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/brainhack101/IntroDL/master/notebooks/2019/Eklund/helper_funcs.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrnYRF9-lXsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from helper_funcs import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgAYTVNKlXsl",
        "colab_type": "text"
      },
      "source": [
        "If you have multiple GPUs you can select a single one of them by setting the visible CUDA device to 0, 1, ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6Eby_82nedN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtLJxriDlXsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ushjqquylXso",
        "colab_type": "text"
      },
      "source": [
        "#### Load data\n",
        "\n",
        "The dataset used for the run is **data/&lt;`image_folder`&gt;**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umXQznNrngRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -nc https://raw.githubusercontent.com/brainhack101/IntroDL/master/notebooks/2019/Eklund/data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhh4Cov_nvhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -uo data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9tN-H7UlXsp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_folder = 'malefemale'\n",
        "data = load_data(subfolder=image_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xomIz34dlXss",
        "colab_type": "text"
      },
      "source": [
        "### Model parameters\n",
        "\n",
        "This CycleGAN implementation allows a lot of freedom on both the training parameters and the network architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcQJERzclXst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = {}\n",
        "\n",
        "# Data\n",
        "opt['channels'] = data[\"nr_of_channels\"]\n",
        "opt['img_shape'] = data[\"image_size\"] + (opt['channels'],)\n",
        "print('Image shape: ', opt['img_shape'])\n",
        "\n",
        "opt['A_train'] = data[\"trainA_images\"]\n",
        "opt['B_train'] = data[\"trainB_images\"]\n",
        "opt['A_test'] = data[\"testA_images\"]\n",
        "opt['B_test'] = data[\"testB_images\"]\n",
        "opt['testA_image_names'] = data[\"testA_image_names\"]\n",
        "opt['testB_image_names'] = data[\"testB_image_names\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze3MSgrslXsw",
        "colab_type": "text"
      },
      "source": [
        "CylceGAN can be used both on paired and unpaired data. The `paired_data` setting affects the presentation of output images as explained above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIXSEhU2lXsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt['paired_data'] = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4ga7qRdlXsy",
        "colab_type": "text"
      },
      "source": [
        "#### Training parameters\n",
        "- `lambda_ABA` and `lambda_BAB` set the importance of the cycle consistency losses in relation to the adversarial loss `lambda_adversarial`\n",
        "- `learning_rate_D` and `learning_rate_G` are the learning rates for the discriminators and generators respectively.\n",
        "- `generator_iterations` and `discriminator_iterations` represent how many times the generators or discriminators will be trained on every batch of images. This is very useful to keep the training of both systems balanced. In this case the discriminators become successful faster than the generators, so we account for this by training the generators 3 times on every batch of images.\n",
        "- `synthetic_pool_size` sets the size of the image pool used for training the discriminators. The image pool has a certain probability of returning a synthetic image from previous iterations, thus forcing the discriminator to have a certain \"memory\". More information on this method can be found in [this paper](https://arxiv.org/abs/1612.07828).\n",
        "- `beta_1` and `beta_2` are paremeters of the [Adam](https://arxiv.org/abs/1412.6980) optimizers used on the generators and discriminators.\n",
        "- `batch_size` determines the number of images used for each update of the network weights. Due to the significant memory requirements of CycleGAN it is difficult to use a large batch size. For the small example dataset values between 1-30 may be possible.\n",
        "- `epochs` sets the number of training epochs. Each epoch goes through all the training images once. The number of epochs necessary to train a model is therefore dependent on both the number of training images available and the batch size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRB3NWjIlXsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training parameters\n",
        "opt['lambda_ABA'] = 10.0  # Cyclic loss weight A_2_B\n",
        "opt['lambda_BAB'] = 10.0  # Cyclic loss weight B_2_A\n",
        "opt['lambda_adversarial'] = 1.0  # Weight for loss from discriminator guess on synthetic images\n",
        "opt['learning_rate_D'] = 2e-4\n",
        "opt['learning_rate_G'] = 2e-4\n",
        "opt['generator_iterations'] = 3  # Number of generator training iterations in each training loop\n",
        "opt['discriminator_iterations'] = 1  # Number of discriminator training iterations in each training loop\n",
        "opt['synthetic_pool_size'] = 50  # Size of image pools used for training the discriminators\n",
        "opt['beta_1'] = 0.5  # Adam parameter\n",
        "opt['beta_2'] = 0.999  # Adam parameter\n",
        "opt['batch_size'] = 10  # Number of images per batch\n",
        "opt['epochs'] = 200  # Choose multiples of 20 since the models are saved each 20th epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXX_5VLwlXs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Output parameters\n",
        "opt['save_models'] = True  # Save or not the generator and discriminator models\n",
        "opt['save_training_img'] = True  # Save or not example training results or only tmp.png\n",
        "opt['save_training_img_interval'] = 1  # Number of epoch between saves of intermediate training results\n",
        "opt['self.tmp_img_update_frequency'] = 3  # Number of batches between updates of tmp.png"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaVHf9TNlXs4",
        "colab_type": "text"
      },
      "source": [
        "#### Architecture parameters\n",
        "- `use_instance_normalization` is supposed to allow the selection of instance normalization or batch normalization layes. At the moment only instance normalization is implemented, so this option does not do anything.\n",
        "- `use_dropout` and `use_bias` allows setting droupout layers in the generators and whether to use a bias term in the various convolutional layer in the genrators and discriminators.\n",
        "- `use_linear_decay` applies linear decay on the learning rates of the generators and discriminators,   `decay_epoch`\n",
        "- `use_patchgan` determines whether the discriminator evaluates the \"realness\" of images on a patch basis or on the whole. More information on PatchGAN can be found in [this paper](https://arxiv.org/abs/1611.07004).\n",
        "- `use_resize_convolution` provides two ways to perfrom the upsampling in the generator, with significant differences in the results. More information can be found in [this article](https://distill.pub/2016/deconv-checkerboard/). Each has its advantages, and we have managed to get successful result with both methods\n",
        "- `use_discriminator sigmoid` adds a sigmoid activation at the end of the discrimintator, forcing its output to the (0-1) range."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVnsr6pSlXs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Architecture parameters\n",
        "opt['use_instance_normalization'] = True  # Use instance normalization or batch normalization\n",
        "opt['use_dropout'] = False  # Dropout in residual blocks\n",
        "opt['use_bias'] = True  # Use bias\n",
        "opt['use_linear_decay'] = True  # Linear decay of learning rate, for both discriminators and generators\n",
        "opt['decay_epoch'] = 101  # The epoch where the linear decay of the learning rates start\n",
        "opt['use_patchgan'] = True  # PatchGAN - if false the discriminator learning rate should be decreased\n",
        "opt['use_resize_convolution'] = False  # Resize convolution - instead of transpose convolution in deconvolution layers (uk) - can reduce checkerboard artifacts but the blurring might affect the cycle-consistency\n",
        "opt['discriminator_sigmoid'] = True  # Add a final sigmoid activation to the discriminator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfOub3aQlXs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tweaks\n",
        "opt['REAL_LABEL'] = 1.0  # Use e.g. 0.9 to avoid training the discriminators to zero loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82fBbXHYlXs9",
        "colab_type": "text"
      },
      "source": [
        "### Model architecture\n",
        "\n",
        "#### Layer blocks\n",
        "These are the individual layer blocks that are used to build the generators and discriminator. More information can be found in the appendix of the [CycleGAN paper](https://arxiv.org/abs/1703.10593)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQuI04BwlXs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Discriminator layers\n",
        "def ck(model, opt, x, k, use_normalization, use_bias):\n",
        "    x = Conv2D(filters=k, kernel_size=4, strides=2, padding='same', use_bias=use_bias)(x)\n",
        "    if use_normalization:\n",
        "        x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    return x\n",
        "\n",
        "# First generator layer\n",
        "def c7Ak(model, opt, x, k):\n",
        "    x = Conv2D(filters=k, kernel_size=7, strides=1, padding='valid', use_bias=opt['use_bias'])(x)\n",
        "    x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "# Downsampling\n",
        "def dk(model, opt, x, k):  # Should have reflection padding\n",
        "    x = Conv2D(filters=k, kernel_size=3, strides=2, padding='same', use_bias=opt['use_bias'])(x)\n",
        "    x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "# Residual block\n",
        "def Rk(model, opt, x0):\n",
        "    k = int(x0.shape[-1])\n",
        "\n",
        "    # First layer\n",
        "    x = ReflectionPadding2D((1,1))(x0)\n",
        "    x = Conv2D(filters=k, kernel_size=3, strides=1, padding='valid', use_bias=opt['use_bias'])(x)\n",
        "    x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if opt['use_dropout']:\n",
        "        x = Dropout(0.5)(x)\n",
        "\n",
        "    # Second layer\n",
        "    x = ReflectionPadding2D((1, 1))(x)\n",
        "    x = Conv2D(filters=k, kernel_size=3, strides=1, padding='valid', use_bias=opt['use_bias'])(x)\n",
        "    x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
        "    # Merge\n",
        "    x = add([x, x0])\n",
        "\n",
        "    return x\n",
        "\n",
        "# Upsampling\n",
        "def uk(model, opt, x, k):\n",
        "    # (up sampling followed by 1x1 convolution <=> fractional-strided 1/2)\n",
        "    if opt['use_resize_convolution']:\n",
        "        x = UpSampling2D(size=(2, 2))(x)  # Nearest neighbor upsampling\n",
        "        x = ReflectionPadding2D((1, 1))(x)\n",
        "        x = Conv2D(filters=k, kernel_size=3, strides=1, padding='valid', use_bias=opt['use_bias'])(x)\n",
        "    else:\n",
        "        x = Conv2DTranspose(filters=k, kernel_size=3, strides=2, padding='same', use_bias=opt['use_bias'])(x)  # this matches fractionally stided with stride 1/2\n",
        "    x = model['normalization'](axis=3, center=True, epsilon=1e-5)(x, training=True)\n",
        "    x = Activation('relu')(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejV66kdmlXtA",
        "colab_type": "text"
      },
      "source": [
        "#### Architecture functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDicsY-PlXtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator(model, opt, name=None):\n",
        "    # Input\n",
        "    input_img = Input(shape=opt['img_shape'])\n",
        "\n",
        "    # Layers 1-4\n",
        "    x = ck(model, opt, input_img, 64, False, True) #  Instance normalization is not used for this layer)\n",
        "    x = ck(model, opt, x, 128, True, opt['use_bias'])\n",
        "    x = ck(model, opt, x, 256, True, opt['use_bias'])\n",
        "    x = ck(model, opt, x, 512, True, opt['use_bias'])\n",
        "\n",
        "    # Layer 5: Output\n",
        "    if opt['use_patchgan']:\n",
        "        x = Conv2D(filters=1, kernel_size=4, strides=1, padding='same', use_bias=True)(x)\n",
        "    else:\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(1)(x)\n",
        "\n",
        "    if opt['discriminator_sigmoid']:\n",
        "        x = Activation('sigmoid')(x)\n",
        "\n",
        "    return Model(inputs=input_img, outputs=x, name=name)\n",
        "\n",
        "def build_generator(model, opt, name=None):\n",
        "    # Layer 1: Input\n",
        "    input_img = Input(shape=opt['img_shape'])\n",
        "    x = ReflectionPadding2D((3, 3))(input_img)\n",
        "    x = c7Ak(model, opt, x, 32)\n",
        "\n",
        "    # Layer 2-3: Downsampling\n",
        "    x = dk(model, opt, x, 64)\n",
        "    x = dk(model, opt, x, 128)\n",
        "\n",
        "    # Layers 4-12: Residual blocks\n",
        "    for _ in range(4, 13):\n",
        "        x = Rk(model, opt, x)\n",
        "\n",
        "    # Layer 13:14: Upsampling\n",
        "    x = uk(model, opt, x, 64)\n",
        "    x = uk(model, opt, x, 32)\n",
        "\n",
        "    # Layer 15: Output\n",
        "    x = ReflectionPadding2D((3, 3))(x)\n",
        "    x = Conv2D(opt['channels'], kernel_size=7, strides=1, padding='valid', use_bias=True)(x)\n",
        "    x = Activation('tanh')(x)\n",
        "\n",
        "    return Model(inputs=input_img, outputs=x, name=name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytd1YAMXlXtF",
        "colab_type": "text"
      },
      "source": [
        "#### Loss functions\n",
        "The discriminators use MSE loss. The generators use MSE for the adversarial losses and MAE for the cycle consistency losses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhESrpK9lXtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mean squared error\n",
        "def mse(y_true, y_pred):\n",
        "    loss = tf.reduce_mean(tf.squared_difference(y_pred, y_true))\n",
        "    return loss\n",
        "\n",
        "# Mean absolute error\n",
        "def mae(y_true, y_pred):\n",
        "    loss = tf.reduce_mean(tf.abs(y_pred - y_true))\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7B6aXKllXtI",
        "colab_type": "text"
      },
      "source": [
        "#### Build CycleGAN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBLv_l3llXtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = {}\n",
        "\n",
        "# Normalization\n",
        "model['normalization'] = InstanceNormalization\n",
        "\n",
        "# Optimizers\n",
        "model['opt_D'] = Adam(opt['learning_rate_D'], opt['beta_1'], opt['beta_2'])\n",
        "model['opt_G'] = Adam(opt['learning_rate_G'], opt['beta_1'], opt['beta_2'])\n",
        "\n",
        "# Build discriminators\n",
        "D_A = build_discriminator(model, opt, name='D_A')\n",
        "D_B = build_discriminator(model, opt, name='D_B')\n",
        "\n",
        "# Define discriminator models\n",
        "image_A = Input(shape=opt['img_shape'])\n",
        "image_B = Input(shape=opt['img_shape'])\n",
        "guess_A = D_A(image_A)\n",
        "guess_B = D_B(image_B)\n",
        "model['D_A'] = Model(inputs=image_A, outputs=guess_A, name='D_A_model')\n",
        "model['D_B'] = Model(inputs=image_B, outputs=guess_B, name='D_B_model')\n",
        "\n",
        "# Compile discriminator models\n",
        "loss_weights_D = [0.5]  # 0.5 since we train on real and synthetic images\n",
        "model['D_A'].compile(optimizer=model['opt_D'],\n",
        "                 loss=mse,\n",
        "                 loss_weights=loss_weights_D)\n",
        "model['D_B'].compile(optimizer=model['opt_D'],\n",
        "                 loss=mse,\n",
        "                 loss_weights=loss_weights_D)\n",
        "\n",
        "# Use containers to make a static copy of discriminators, used when training the generators\n",
        "model['D_A_static'] = Network(inputs=image_A, outputs=guess_A, name='D_A_static_model')\n",
        "model['D_B_static'] = Network(inputs=image_B, outputs=guess_B, name='D_B_static_model')\n",
        "\n",
        "# Do not update discriminator weights during generator training\n",
        "model['D_A_static'].trainable = False\n",
        "model['D_B_static'].trainable = False\n",
        "\n",
        "# Build generators\n",
        "model['G_A2B'] = build_generator(model, opt, name='G_A2B_model')\n",
        "model['G_B2A'] = build_generator(model, opt, name='G_B2A_model')\n",
        "\n",
        "# Define full CycleGAN model, used for training the generators\n",
        "real_A = Input(shape=opt['img_shape'], name='real_A')\n",
        "real_B = Input(shape=opt['img_shape'], name='real_B')\n",
        "synthetic_B = model['G_A2B'](real_A)\n",
        "synthetic_A = model['G_B2A'](real_B)\n",
        "dB_guess_synthetic = model['D_B_static'](synthetic_B)\n",
        "dA_guess_synthetic = model['D_A_static'](synthetic_A)\n",
        "reconstructed_A = model['G_B2A'](synthetic_B)\n",
        "reconstructed_B = model['G_A2B'](synthetic_A)\n",
        "\n",
        "# Compile full CycleGAN model\n",
        "model_outputs = [reconstructed_A, reconstructed_B,\n",
        "                 dB_guess_synthetic, dA_guess_synthetic]\n",
        "compile_losses = [mae, mae,\n",
        "                  mse, mse]\n",
        "compile_weights = [opt['lambda_ABA'], opt['lambda_BAB'],\n",
        "                   opt['lambda_adversarial'], opt['lambda_adversarial']]\n",
        "\n",
        "model['G_model'] = Model(inputs=[real_A, real_B],\n",
        "                     outputs=model_outputs,\n",
        "                     name='G_model')\n",
        "\n",
        "model['G_model'].compile(optimizer=model['opt_G'],\n",
        "                     loss=compile_losses,\n",
        "                     loss_weights=compile_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPgzq3VjlXtM",
        "colab_type": "text"
      },
      "source": [
        "#### Folders and configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3yDA_O7lXtN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt['date_time'] = time.strftime('%Y%m%d-%H%M%S', time.localtime()) + '-' + image_folder\n",
        "\n",
        "# Output folder for run data and images\n",
        "opt['out_dir'] = os.path.join('images', opt['date_time'])\n",
        "if not os.path.exists(opt['out_dir']):\n",
        "    os.makedirs(opt['out_dir'])\n",
        "\n",
        "# Output folder for saved models\n",
        "if opt['save_models']:\n",
        "    opt['model_out_dir'] = os.path.join('saved_models', opt['date_time'])\n",
        "    if not os.path.exists(opt['model_out_dir']):\n",
        "        os.makedirs(opt['model_out_dir'])\n",
        "\n",
        "write_metadata_to_JSON(model, opt)\n",
        "\n",
        "# Don't pre-allocate GPU memory; allocate as-needed\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "K.tensorflow_backend.set_session(tf.Session(config=config))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk04cLeklXtQ",
        "colab_type": "text"
      },
      "source": [
        "### Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gi4VtqjlXtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, opt):\n",
        "\n",
        "    def run_training_batch():\n",
        "\n",
        "        # ======= Discriminator training ======\n",
        "        # Generate batch of synthetic images\n",
        "        synthetic_images_B = model['G_A2B'].predict(real_images_A)\n",
        "        synthetic_images_A = model['G_B2A'].predict(real_images_B)\n",
        "        synthetic_images_B = synthetic_pool_B.query(synthetic_images_B)\n",
        "        synthetic_images_A = synthetic_pool_A.query(synthetic_images_A)\n",
        "\n",
        "        # Train discriminators on batch\n",
        "        D_loss = []\n",
        "        for _ in range(opt['discriminator_iterations']):\n",
        "            D_A_loss_real = model['D_A'].train_on_batch(x=real_images_A, y=ones)\n",
        "            D_B_loss_real = model['D_B'].train_on_batch(x=real_images_B, y=ones)\n",
        "            D_A_loss_synthetic = model['D_A'].train_on_batch(x=synthetic_images_A, y=zeros)\n",
        "            D_B_loss_synthetic = model['D_B'].train_on_batch(x=synthetic_images_B, y=zeros)\n",
        "            D_A_loss = D_A_loss_real + D_A_loss_synthetic\n",
        "            D_B_loss = D_B_loss_real + D_B_loss_synthetic\n",
        "            D_loss.append(D_A_loss + D_B_loss)\n",
        "\n",
        "        # ======= Generator training ==========\n",
        "        target_data = [real_images_A, real_images_B, ones, ones]  # Reconstructed images need to match originals, discriminators need to predict ones\n",
        "\n",
        "        # Train generators on batch\n",
        "        G_loss = []\n",
        "        for _ in range(opt['generator_iterations']):\n",
        "            G_loss.append(model['G_model'].train_on_batch(\n",
        "                x=[real_images_A, real_images_B], y=target_data))\n",
        "\n",
        "        # =====================================\n",
        "\n",
        "        # Update learning rates\n",
        "        if opt['use_linear_decay'] and epoch >= opt['decay_epoch']:\n",
        "            update_lr(model['D_A'], decay_D)\n",
        "            update_lr(model['D_B'], decay_D)\n",
        "            update_lr(model['G_model'], decay_G)\n",
        "\n",
        "        # Store training losses\n",
        "        D_A_losses.append(D_A_loss)\n",
        "        D_B_losses.append(D_B_loss)\n",
        "        D_losses.append(D_loss[-1])\n",
        "\n",
        "        ABA_reconstruction_loss = G_loss[-1][1]\n",
        "        BAB_reconstruction_loss = G_loss[-1][2]\n",
        "        reconstruction_loss = ABA_reconstruction_loss + BAB_reconstruction_loss\n",
        "        G_AB_adversarial_loss = G_loss[-1][3]\n",
        "        G_BA_adversarial_loss = G_loss[-1][4]\n",
        "\n",
        "        ABA_reconstruction_losses.append(ABA_reconstruction_loss)\n",
        "        BAB_reconstruction_losses.append(BAB_reconstruction_loss)\n",
        "        reconstruction_losses.append(reconstruction_loss)\n",
        "        G_AB_adversarial_losses.append(G_AB_adversarial_loss)\n",
        "        G_BA_adversarial_losses.append(G_BA_adversarial_loss)\n",
        "        G_losses.append(G_loss[-1][0])\n",
        "\n",
        "        # Print training status\n",
        "        print('\\n')\n",
        "        print('Epoch ---------------------', epoch, '/', opt['epochs'])\n",
        "        print('Loop index ----------------', loop_index + 1, '/', nr_im_per_epoch)\n",
        "        if opt['discriminator_iterations'] > 1:\n",
        "            print('  Discriminator losses:')\n",
        "            for i in range(opt['discriminator_iterations']):\n",
        "                print('D_loss', D_loss[i])\n",
        "        if opt['generator_iterations'] > 1:\n",
        "            print('  Generator losses:')\n",
        "            for i in range(opt['generator_iterations']):\n",
        "                print('G_loss', G_loss[i])\n",
        "        print('  Summary:')\n",
        "        print('D_lr:', K.get_value(model['D_A'].optimizer.lr))\n",
        "        print('G_lr', K.get_value(model['G_model'].optimizer.lr))\n",
        "        print('D_loss: ', D_loss[-1])\n",
        "        print('G_loss: ', G_loss[-1][0])\n",
        "        print('reconstruction_loss: ', reconstruction_loss)\n",
        "        print_ETA(opt, start_time, epoch, nr_im_per_epoch, loop_index)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        if loop_index % 3*opt['batch_size'] == 0:\n",
        "            # Save temporary images continously\n",
        "            save_tmp_images(model, opt, real_images_A[0], real_images_B[0],\n",
        "                                 synthetic_images_A[0], synthetic_images_B[0])\n",
        "\n",
        "    # ======================================================================\n",
        "    # Begin training\n",
        "    # ======================================================================\n",
        "    if opt['save_training_img'] and not os.path.exists(os.path.join(opt['out_dir'], 'train_A')):\n",
        "        os.makedirs(os.path.join(opt['out_dir'], 'train_A'))\n",
        "        os.makedirs(os.path.join(opt['out_dir'], 'train_B'))\n",
        "        os.makedirs(os.path.join(opt['out_dir'], 'test_A'))\n",
        "        os.makedirs(os.path.join(opt['out_dir'], 'test_B'))\n",
        "\n",
        "    D_A_losses = []\n",
        "    D_B_losses = []\n",
        "    D_losses = []\n",
        "\n",
        "    ABA_reconstruction_losses = []\n",
        "    BAB_reconstruction_losses = []\n",
        "    reconstruction_losses = []\n",
        "    G_AB_adversarial_losses = []\n",
        "    G_BA_adversarial_losses = []\n",
        "    G_losses = []\n",
        "\n",
        "    # Image pools used to update the discriminators\n",
        "    synthetic_pool_A = ImagePool(opt['synthetic_pool_size'])\n",
        "    synthetic_pool_B = ImagePool(opt['synthetic_pool_size'])\n",
        "\n",
        "    # Labels used for discriminator training\n",
        "    label_shape = (opt['batch_size'],) + model['D_A'].output_shape[1:]\n",
        "    ones = np.ones(shape=label_shape) * opt['REAL_LABEL']\n",
        "    zeros = ones * 0\n",
        "\n",
        "    # Linear learning rate decay\n",
        "    if opt['use_linear_decay']:\n",
        "        decay_D, decay_G = get_lr_linear_decay_rate(opt)\n",
        "\n",
        "    nr_train_im_A = opt['A_train'].shape[0]\n",
        "    nr_train_im_B = opt['B_train'].shape[0]\n",
        "    nr_im_per_epoch = int(np.ceil(np.max((nr_train_im_A, nr_train_im_B)) / opt['batch_size']) * opt['batch_size'])\n",
        "\n",
        "    # Start stopwatch for ETAs\n",
        "    start_time = time.time()\n",
        "    timer_started = False\n",
        "\n",
        "    for epoch in range(1, opt['epochs'] + 1):\n",
        "        # random_order_A = np.random.randint(nr_train_im_A, size=nr_im_per_epoch)\n",
        "        # random_order_B = np.random.randint(nr_train_im_B, size=nr_im_per_epoch)\n",
        "\n",
        "        random_order_A = np.concatenate((np.random.permutation(nr_train_im_A),\n",
        "                                         np.random.randint(nr_train_im_A, size=nr_im_per_epoch - nr_train_im_A)))\n",
        "        random_order_B = np.concatenate((np.random.permutation(nr_train_im_B),\n",
        "                                         np.random.randint(nr_train_im_B, size=nr_im_per_epoch - nr_train_im_B)))\n",
        "\n",
        "        # Train on image batch\n",
        "        for loop_index in range(0, nr_im_per_epoch, opt['batch_size']):\n",
        "            indices_A = random_order_A[loop_index:loop_index + opt['batch_size']]\n",
        "            indices_B = random_order_B[loop_index:loop_index + opt['batch_size']]\n",
        "\n",
        "            real_images_A = opt['A_train'][indices_A]\n",
        "            real_images_B = opt['B_train'][indices_B]\n",
        "\n",
        "            # Train on image batch\n",
        "            run_training_batch()\n",
        "\n",
        "            # Start timer after first (slow) iteration has finished\n",
        "            if not timer_started:\n",
        "                start_time = time.time()\n",
        "                timer_started = True\n",
        "\n",
        "        # Save training images\n",
        "        if opt['save_training_img'] and epoch % opt['save_training_img_interval'] == 0:\n",
        "            print('\\n', '\\n', '-------------------------Saving images for epoch', epoch, '-------------------------', '\\n', '\\n')\n",
        "            save_epoch_images(model, opt, epoch)\n",
        "\n",
        "        # Save model\n",
        "        if opt['save_models'] and epoch % 20 == 0:\n",
        "            save_model(opt, model['D_A'], epoch)\n",
        "            save_model(opt, model['D_B'], epoch)\n",
        "            save_model(opt, model['G_A2B'], epoch)\n",
        "            save_model(opt, model['G_B2A'], epoch)\n",
        "\n",
        "        # Save training history\n",
        "        training_history = {\n",
        "            'DA_losses': D_A_losses,\n",
        "            'DB_losses': D_B_losses,\n",
        "            'G_AB_adversarial_losses': G_AB_adversarial_losses,\n",
        "            'G_BA_adversarial_losses': G_BA_adversarial_losses,\n",
        "            'ABA_reconstruction_losses': ABA_reconstruction_losses,\n",
        "            'BAB_reconstruction_losses': BAB_reconstruction_losses,\n",
        "            'reconstruction_losses': reconstruction_losses,\n",
        "            'D_losses': D_losses,\n",
        "            'G_losses': G_losses}\n",
        "        write_loss_data_to_file(opt, training_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G-fVM6flXtW",
        "colab_type": "text"
      },
      "source": [
        "### Train CycleGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_0XjcQ3lXtW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(model, opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoWx9DDylXtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}